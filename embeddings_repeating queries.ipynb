{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tatianamgar/Work_GPN/blob/main/embeddings_repeating%20queries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Импорты**"
      ],
      "metadata": {
        "id": "jLaPddF8l672"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plWWJM2rtbAy"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ruamel.yaml\n"
      ],
      "metadata": {
        "id": "TK8u4hXinUeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "id": "Y2CTdFlLnVBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDkrEaqyp1-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40899734-852b-44c2-bbe8-b8a377fa7cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "root_dir = os.path.abspath(os.getcwd())\n",
        "root_dir = str(Path(root_dir).parent)\n",
        "sys.path.append(root_dir)\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import torch\n",
        "from transformers import DistilBertForTokenClassification, DistilBertTokenizerFast\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = stopwords.words(\"russian\")\n",
        "\n",
        "from src.utilits import read_config\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Функции**"
      ],
      "metadata": {
        "id": "pKr_Qm7vnEFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t24daB06p1-I"
      },
      "outputs": [],
      "source": [
        "config_path = \"params.yaml\"\n",
        "config = read_config.read(config_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWY6JdT6p1-K"
      },
      "outputs": [],
      "source": [
        "class Embed:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.batch_size = config[\"train_parameters\"][\"bs\"]\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.labels = self.__load_tags(config[\"files\"][\"tag_file\"])\n",
        "        num_tags = len(self.labels)\n",
        "\n",
        "        self.model = DistilBertForTokenClassification.from_pretrained(\n",
        "            config[\"files\"][\"path_model_save\"],\n",
        "            num_labels=num_tags,\n",
        "            output_attentions=False,\n",
        "            output_hidden_states=False,\n",
        "        )\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        self.tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
        "            config[\"files\"][\"tokenizer_model\"], do_lower_case=False,\n",
        "        )\n",
        "\n",
        "    def get(self, data: list):\n",
        "        embeddings = []\n",
        "        for i in range(0, len(data), self.batch_size):\n",
        "            batch = data[i:i+self.batch_size]\n",
        "            encodings = self.tokenizer(\n",
        "                batch,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "                return_offsets_mapping=True,\n",
        "            )\n",
        "            input_id = encodings.input_ids.to(self.device)\n",
        "            attention_mask = encodings.attention_mask.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                batch_embeddings = self.model.distilbert(input_id, attention_mask=attention_mask)[0].cpu().numpy()\n",
        "            embeddings.append(batch_embeddings.mean(axis=1))\n",
        "        return np.concatenate(embeddings, axis=0)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def __load_tags(tags_path):\n",
        "        with open(tags_path, \"rb\") as f:\n",
        "            tags = pickle.load(f)\n",
        "        return [tag.replace(\"B-\", \"\").replace(\"I-\", \"\") for tag in tags]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed = Embed(config)\n",
        "def vectorize_texts_bert(texts):\n",
        "    embeddings = embed.get(texts)\n",
        "\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "D-F9_UuoouUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# загружаем модель USE\n",
        "nlp = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
        "\n",
        "def vectorize_texts_use(texts):\n",
        "    vecs = nlp(texts)\n",
        "    return vecs"
      ],
      "metadata": {
        "id": "ljpoKekGIPtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-qsclFhB9zg"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # пунктуация и пробелы\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub('[\\s]+', ' ', text)\n",
        "\n",
        "    tokens = text.split(\" \")\n",
        "\n",
        "\n",
        "    # стоп-слова и стемминг\n",
        "    # no_stop_lemm_tokens = [stemmer.stem(token) for token in tokens if token not in stopwords]\n",
        "\n",
        "    return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def repeated_queries_search(sessions, model = 'bert'):\n",
        "\n",
        "\n",
        "    sessions['sims_to_prev'] = '[]'\n",
        "\n",
        "    for ind in tqdm(sessions.index):\n",
        "        # preprocessing\n",
        "        pre_list = sessions['query'][ind]\n",
        "        iter_list = list(map(preprocess_text, pre_list))\n",
        "\n",
        "        # векторизация и сходство\n",
        "        if model == 'bert':\n",
        "            vectors = vectorize_texts_bert(iter_list)\n",
        "        elif model == 'use':\n",
        "            vectors = vectorize_texts_use(iter_list)\n",
        "\n",
        "        smlr = cosine_similarity(vectors, vectors)\n",
        "\n",
        "        # подготовка итогового листа для дф\n",
        "        indx_query = 0\n",
        "        smlr_lst = []\n",
        "        for ndx, lst in enumerate(smlr):\n",
        "            if ndx == 0:\n",
        "                smlr_lst.append(0)\n",
        "                continue\n",
        "\n",
        "            else:\n",
        "                smlr_lst.append(smlr[ndx][indx_query])\n",
        "                indx_query += 1\n",
        "\n",
        "        sessions['sims_to_prev'][ind] = list(smlr_lst)\n",
        "\n",
        "    return sessions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BF85WHbKqCsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_df_with_sim(sessions):\n",
        "\n",
        "    result = pd.DataFrame(columns=['similarity with previous query', 'index', 'query', 'session_id'])\n",
        "\n",
        "    for ind in sessions.index:\n",
        "        df = pd.DataFrame({'similarity with previous query': sessions['sims_to_prev'][ind], 'index' : sessions['level_0'][ind],\n",
        "                            'query': sessions['query'][ind], 'session_id': sessions['session_id'][ind]})\n",
        "        result = pd.concat([result, df])\n",
        "\n",
        "    result = result.reset_index()\n",
        "    return result\n",
        "\n"
      ],
      "metadata": {
        "id": "9STaXsXRtov-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_fpL7pxB9zc"
      },
      "source": [
        "# **Обработка выгрузки**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDng3easB9zd"
      },
      "source": [
        "**Предобработка**\n",
        "убрать из выборки (здесь предобработки нет, т.к. я передаю уже обработанный датасет для работы в Колабе, в кспд соединю с другим скриптом обработки):\n",
        "- запросы без айди сессий и запросов\n",
        "- пустые запросы\n",
        "- поиск по ID\n",
        "- запросы, совершенные с помощью кнопки \"похожие материалы\" (в самом подборе уточняющих запросов будет учтено)\n",
        "\n",
        "Подбор утоняющих запросов:\n",
        "- сортировать запросы по времени в каждом айди сессии по возрастанию\n",
        "- цикл векторизации и кос сходства на лист запросов в каждой сессии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1n6wuAmB9zc"
      },
      "outputs": [],
      "source": [
        "# audit_search таблица\n",
        "search = pd.read_csv('/content/search_processed.csv', sep='$')\n",
        "search_sorted = search.sort_values(['session_id', 'date_time'],ascending=True).reset_index()\n",
        "sessions = search_sorted.groupby(\"session_id\").agg({\"query\": lambda x: list(x), 'level_0': lambda y: list(y)}).reset_index()\n",
        "sessions.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> # **USE**"
      ],
      "metadata": {
        "id": "NBQV6ymLIVQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sessions_use = repeated_queries_search(sessions, model = 'use')\n",
        "result = create_df_with_sim(sessions_use)\n",
        "search_sims = pd.merge(search_sorted, result, left_on = ['level_0'], right_index=True)\n",
        "search_sims = search_sims[['date_time', 'page_number', 'query_x', 'username', 'total_results', 'activeLayer', 'session_id_x', 'similarity with previous query']]\n",
        "search_sims.head(3)"
      ],
      "metadata": {
        "id": "kZzItR-GxJsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_sims.to_excel('sim_by_use.xlsx')"
      ],
      "metadata": {
        "id": "BRCyFgew2SLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> # **BERT**"
      ],
      "metadata": {
        "id": "S_hcgtZa1UGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sessions_bert = repeated_queries_search(sessions)\n",
        "result = create_df_with_sim(sessions_bert)\n",
        "search_sims = pd.merge(search_sorted, result, left_on = ['level_0'], right_index=True)\n",
        "search_sims = search_sims[['date_time', 'page_number', 'query_x', 'username', 'total_results', 'activeLayer', 'session_id_x', 'similarity with previous query']]\n",
        "search_sims.head(3)"
      ],
      "metadata": {
        "id": "Z81gnMTs1Xku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "! первый запрос в сессии проставляется сходством 0"
      ],
      "metadata": {
        "id": "X1_Pw41iNQp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_sims.to_excel('sim_by_bert.xlsx')"
      ],
      "metadata": {
        "id": "c51X_tTw4gxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P7iQwzDc7WbX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jLaPddF8l672",
        "pKr_Qm7vnEFA",
        "7_fpL7pxB9zc",
        "NBQV6ymLIVQ7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "nsi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}